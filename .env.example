# ===========================================
# API Configuration
# ===========================================

# API Key (required for OpenAI, optional for local models like Ollama)
OPENAI_API_KEY=your_api_key_here

# Base URL for OpenAI-compatible APIs (optional)
# Leave empty for OpenAI, or set to your provider's endpoint
# Examples:
#   Ollama: http://localhost:11434/v1
#   LM Studio: http://localhost:1234/v1
#   vLLM: http://localhost:8000/v1
#   OpenRouter: https://openrouter.ai/api/v1
OPENAI_BASE_URL=

# Provider name for display in UI (optional, defaults to "OpenAI")
PROVIDER_NAME=OpenAI

# Available models (comma-separated list, optional)
# If not set, defaults to OpenAI models
# Examples:
#   OpenAI: gpt-4o,gpt-4o-mini,gpt-4-turbo,gpt-3.5-turbo
#   Ollama: llama3.2,mistral,codellama,phi3
#   Custom: my-model-1,my-model-2,my-custom-model
AVAILABLE_MODELS=

# Default model to use for testing prompts (optional)
# Must be one of the models from AVAILABLE_MODELS
# Example: gpt-4o or llama3.2
DEFAULT_MODEL=

# Default model parameters (optional)
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=1000

# ===========================================
# Example Configurations for Different Providers
# ===========================================

# --- OpenAI (default) ---
# OPENAI_API_KEY=sk-...
# OPENAI_BASE_URL=
# PROVIDER_NAME=OpenAI
# AVAILABLE_MODELS=gpt-4o,gpt-4o-mini,gpt-4-turbo,gpt-3.5-turbo
# DEFAULT_MODEL=gpt-4o

# --- Ollama (local) ---
# OPENAI_API_KEY=not-needed
# OPENAI_BASE_URL=http://localhost:11434/v1
# PROVIDER_NAME=Ollama
# AVAILABLE_MODELS=llama3.2,mistral,codellama,phi3
# DEFAULT_MODEL=llama3.2

# --- LM Studio (local) ---
# OPENAI_API_KEY=not-needed
# OPENAI_BASE_URL=http://localhost:1234/v1
# PROVIDER_NAME=LM Studio
# AVAILABLE_MODELS=
# DEFAULT_MODEL=

# --- OpenRouter ---
# OPENAI_API_KEY=sk-or-v1-...
# OPENAI_BASE_URL=https://openrouter.ai/api/v1
# PROVIDER_NAME=OpenRouter
# AVAILABLE_MODELS=anthropic/claude-3.5-sonnet,openai/gpt-4o,meta-llama/llama-3.2-90b
# DEFAULT_MODEL=anthropic/claude-3.5-sonnet

# --- vLLM (self-hosted) ---
# OPENAI_API_KEY=not-needed
# OPENAI_BASE_URL=http://localhost:8000/v1
# PROVIDER_NAME=vLLM
# AVAILABLE_MODELS=
# DEFAULT_MODEL=
